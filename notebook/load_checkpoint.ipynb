{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/naoki/Documents/GAN_practice/training_checkpoints\"\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naoki/Documents/GAN_practice/training_checkpoints/ckpt-6\n"
     ]
    }
   ],
   "source": [
    "print(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# no need to rewrite(just import)\n",
    "# network module\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer\n",
    "                                               , use_bias=False))\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.down_stack = [\n",
    "            downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
    "            downsample(128, 4),  # (bs, 64, 64, 128)\n",
    "            downsample(256, 4),  # (bs, 32, 32, 256)\n",
    "            downsample(512, 4),  # (bs, 16, 16, 512)\n",
    "            downsample(512, 4),  # (bs, 8, 8, 512)\n",
    "            downsample(512, 4),  # (bs, 4, 4, 512)\n",
    "            downsample(512, 4),  # (bs, 2, 2, 512)\n",
    "            downsample(512, 4),  # (bs, 1, 1, 512)\n",
    "        ]\n",
    "\n",
    "        self.up_stack = [\n",
    "            upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "            upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "            upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "            upsample(512, 4),  # (bs, 16, 16, 1024\n",
    "            upsample(256, 4),  # (bs, 32, 32, 512)\n",
    "            upsample(128, 4),  # (bs, 64, 64, 256)\n",
    "            upsample(64, 4),  # (bs, 128, 128, 128)\n",
    "        ]\n",
    "\n",
    "        self.last = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same',\n",
    "                                                    kernel_initializer=tf.random_normal_initializer(0, 0.02),\n",
    "                                                    activation='tanh')  # (bs, 256, 256, 3)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = inputs\n",
    "\n",
    "        skips = []\n",
    "        for down in self.down_stack:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "        output = self.last(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.down_stack = [downsample(64, 4, False), downsample(128, 4), downsample(256, 4)]\n",
    "        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=tf.random_normal_initializer(0, 0.02),\n",
    "                                           use_bias=False)\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU()\n",
    "        self.last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=tf.random_normal_initializer(0, 0.02))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs = input + target(concatenated)\n",
    "        inp, tar = inputs\n",
    "        x = tf.concat([inp, tar], 0)\n",
    "        for down in self.down_stack:\n",
    "            x = down(x)\n",
    "        x = self.conv(self.zero_pad1(x))\n",
    "        x = self.zero_pad2(self.leaky_relu(self.batchnorm(x)))\n",
    "        output = self.last(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Nothing to load. No dependencies have been added to <__main__.Generator object at 0x7f7dc104cc10> yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a53847688187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0;31m# streaming restore for any variables created in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0mtrackable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m       \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\u001b[0m in \u001b[0;36massert_nontrivial_match\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m         raise AssertionError(\n\u001b[1;32m    804\u001b[0m             \u001b[0;34m\"Nothing to load. No dependencies have been added to %s yet.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             (self._graph_view.root,))\n\u001b[0m\u001b[1;32m    806\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Nothing to load. No dependencies have been added to <__main__.Generator object at 0x7f7dc104cc10> yet."
     ]
    }
   ],
   "source": [
    "generator.load_weights(latest)\n",
    "weights = generator.get_weights()\n",
    "print(\"**************************\")\n",
    "print(weights)\n",
    "print(\"**************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/naoki/anaconda3/envs/tensorflow-new/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Tensorflow-new",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "tensorflow-new"
  },
  "name": "Untitled2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
