{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class create_input:\n",
    "    def Cutout(self, img):\n",
    "        # choose one or three rectangle with random size(up to 1/5 image size)\n",
    "        print(tf.shape(img))\n",
    "        height = tf.shape(img)[0]\n",
    "        width = tf.shape(img)[1]\n",
    "        h = tf.random.uniform([], int(height/10), int(height/5), tf.int32)\n",
    "        w = tf.random.uniform([], int(width/10), int(width/5), tf.int32)\n",
    "\n",
    "        x1 = tf.random.uniform([], 0, height+1 - h, tf.int32)\n",
    "        y1 = tf.random.uniform([], 0, width+1 - w, tf.int32)\n",
    "\n",
    "        mean = int(tf.reduce_mean(img))\n",
    "        erase_area = tf.cast(tf.fill([h, w, 3], 230), tf.uint8)\n",
    "        erase_img = np.array(img)\n",
    "        erase_img[x1:x1+h, y1:y1+w, :] = erase_area\n",
    "\n",
    "        return erase_img\n",
    "\n",
    "    def WhiteNoise(self, img):\n",
    "        noise = tf.random.normal(tf.shape(img), mean=0.0, stddev=20.0, dtype=tf.float32)\n",
    "        noise_img = noise+img\n",
    "        noise_img = tf.clip_by_value(noise_img, 0.0, 255.0)\n",
    "\n",
    "        return noise_img\n",
    "\n",
    "    def SaltNoise(self, img):\n",
    "        # add salt and pepper noise(1/16 area)\n",
    "        pts_x_white = np.random.randint(0, tf.shape(img)[0]-1, int(tf.shape(img)[0]**2/16))\n",
    "        pts_y_white = np.random.randint(0, tf.shape(img)[1]-1, int(tf.shape(img)[0]**2/16))\n",
    "\n",
    "        pts_x_black = np.random.randint(0, tf.shape(img)[0]-1, int(tf.shape(img)[0]**2/16))\n",
    "        pts_y_black = np.random.randint(0, tf.shape(img)[1]-1, int(tf.shape(img)[0]**2/16))\n",
    "\n",
    "        img = np.array(img)\n",
    "        img[pts_x_white, pts_y_white] = [255, 255, 255]\n",
    "        img[pts_x_black, pts_y_black] = [0, 0, 0]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def fit(self, img):\n",
    "        augs = [self.Cutout_tf, self.WhiteNoise_tf, self.SaltNoise_tf]\n",
    "\n",
    "        num = np.random.randint(low=1, high=len(augs))\n",
    "        selected_augs = random.sample(augs, num)\n",
    "        # for aug in selected_augs:\n",
    "        #     img = aug(img)\n",
    "        img = self.Cutout_tf(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @tf.function\n",
    "    def Cutout_tf(self, img):\n",
    "        img = tf.py_function(self.Cutout, [img], [tf.float32])\n",
    "        return img\n",
    "\n",
    "    @tf.function\n",
    "    def WhiteNoise_tf(self, img):\n",
    "        img = tf.py_function(self.WhiteNoise, [img], [tf.float32])\n",
    "        return img\n",
    "\n",
    "    @tf.function\n",
    "    def SaltNoise_tf(self, img):\n",
    "        img = tf.py_function(self.SaltNoise, [img], [tf.float32])\n",
    "        return img\n",
    "\n",
    "    @tf.function\n",
    "    def fit_tf(self, img):\n",
    "        img = tf.py_function(self.fit, [img], [tf.float32])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class data_aug:\n",
    "    def resize(self, input_image, real_image, height, width):\n",
    "        input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        real_image = tf.image.resize(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        return input_image, real_image\n",
    "\n",
    "    def random_crop(self, input_image, real_image):\n",
    "        stack_image = tf.stack([input_image, real_image], axis=0)\n",
    "        cropped_image = tf.image.random_crop(stack_image, size=[2, 256, 256, 3])\n",
    "\n",
    "        return cropped_image[0], cropped_image[1]\n",
    "\n",
    "    @tf.function()\n",
    "    def random_jitter(self, input_image, real_image):\n",
    "        input_image, real_image = self.resize(input_image, real_image, 286, 286)\n",
    "        input_image, real_image = self.random_crop(input_image, real_image)\n",
    "\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            input_image = tf.image.flip_left_right(input_image)\n",
    "            real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "        return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"PartitionedCall:0\", shape=(286, 286, 3), dtype=float32)\n",
      "Tensor(\"StatefulPartitionedCall:0\", dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-57-18d061b33c91>:56 load_image_train  *\n        input_image, real_image = random_jitter(input_image, real_image)\n    <ipython-input-43-2a3ecff6315b>:36 random_jitter  *\n        input_image, real_image = resize(input_image, real_image, 286, 286)\n    <ipython-input-43-2a3ecff6315b>:22 resize  *\n        input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    /home/naoki/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1357 resize_images_v2\n        skip_resize_if_same=False)\n    /home/naoki/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1072 _resize_images_common\n        raise ValueError('\\'images\\' contains no shape.')\n\n    ValueError: 'images' contains no shape.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-18d061b33c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtrain_dataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_image_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mfirst_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# sample_images = first_batch[0].numpy().reshape((-1, 286, 286))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1591\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3926\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-57-18d061b33c91>:56 load_image_train  *\n        input_image, real_image = random_jitter(input_image, real_image)\n    <ipython-input-43-2a3ecff6315b>:36 random_jitter  *\n        input_image, real_image = resize(input_image, real_image, 286, 286)\n    <ipython-input-43-2a3ecff6315b>:22 resize  *\n        input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    /home/naoki/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1357 resize_images_v2\n        skip_resize_if_same=False)\n    /home/naoki/anaconda3/envs/tensorflow-new/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1072 _resize_images_common\n        raise ValueError('\\'images\\' contains no shape.')\n\n    ValueError: 'images' contains no shape.\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "@tf.function\n",
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image\n",
    "\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "def random_jitter(input_image, real_image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "      # random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_train(image_file):\n",
    "    aug = data_aug()\n",
    "    cr_input = create_input()\n",
    "    input_image = load(image_file)\n",
    "    real_image = load(image_file)\n",
    "    print(input_image)\n",
    "    input_image = cr_input.fit(input_image)[0]\n",
    "    print(input_image)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    aug = data_aug()\n",
    "    cr_input = create_input()\n",
    "    real_image = load(image_file)\n",
    "\n",
    "    input_image = cr_input.fit(real_image)\n",
    "\n",
    "    input_image, real_image = aug.resize(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "train_dataset_path = os.listdir('data/train/')\n",
    "train_dataset_path = [os.path.join(os.path.abspath('data/train/'), p) for p in train_dataset_path]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset_path)\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1)\n",
    "first_batch = next(iter(train_dataset))\n",
    "# sample_images = first_batch[0].numpy().reshape((-1, 286, 286))\n",
    "# sample_image = first_batch[0].numpy().reshape((286, 286, 3))\n",
    "# cv2.imwrite(\"/home/naoki/strange.png\", sample_image)\n",
    "# array_obj = plt.imread(\"/home/naoki/strange.png\")\n",
    "# print(array_obj.shape)\n",
    "# plt.imshow(array_obj)\n",
    "# print(sample_image==array_obj)\n",
    "# train_dataset = tf.data.Dataset.list_files(\"/home/naoki/Documents/GAN_practice/data/train/*.png\")\n",
    "# print(type(train_dataset))\n",
    "# train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "# train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# test_dataset = tf.data.Dataset.list_files('data/test/*.png')\n",
    "# test_dataset = test_dataset.map(load_image_test)\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "@tf.function\n",
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image\n",
    "\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "def random_jitter(input_image, real_image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "      # random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_train(image_file):\n",
    "    input_image = load(image_file)\n",
    "    real_image = load(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "train_dataset_path = os.listdir('data/train/')\n",
    "train_dataset_path = [os.path.join(os.path.abspath('data/train/'), p) for p in train_dataset_path]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset_path)\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "a = ['abcdefg', 'abcdefghijklmn']\n",
    "a = tf.data.Dataset.from_tensor_slices(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "(None, None, 3)\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/home/naoki/ros/aerial_robot_demo_ws/src/aerial_robot_demo/mbzirc2020/mbzirc2020_task2/mbzirc2020_task2_tasks/data/all/\"\n",
    "files = os.listdir(img_path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(img_path, f))]  #  all image names\n",
    "random.shuffle(files_file)\n",
    "files_file_custom = [img_path+files_file[i] for i in range(len(files_file))]  #  all image path\n",
    "print(type(files_file_custom))\n",
    "def load_image(path):\n",
    "    print(path)\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.io.decode_png(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # image = tf.image.resize(image, (224, 224))\n",
    "    print(image.shape)\n",
    "    return image\n",
    "SPLIT_NUM = 0.7\n",
    "split_idx = int(0.7*len(files_file))\n",
    "\n",
    "train_image_path_ds = tf.data.Dataset.from_tensor_slices(files_file_custom[0:split_idx])\n",
    "train_image_ds = train_image_path_ds.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(type(train_image_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [i for i in range(5)]\n",
    "b = [1, 2]\n",
    "c = a+b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.down_stack = [\n",
    "            downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "            downsample(128, 4), # (bs, 64, 64, 128)\n",
    "            downsample(256, 4), # (bs, 32, 32, 256)\n",
    "            downsample(512, 4), # (bs, 16, 16, 512)\n",
    "            downsample(512, 4), # (bs, 8, 8, 512)\n",
    "            downsample(512, 4), # (bs, 4, 4, 512)\n",
    "            downsample(512, 4), # (bs, 2, 2, 512)\n",
    "            downsample(512, 4), # (bs, 1, 1, 512)\n",
    "        ]\n",
    "\n",
    "        self.up_stack = [\n",
    "            upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "            upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "            upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "            upsample(512, 4), # (bs, 16, 16, 1024\n",
    "            upsample(256, 4), # (bs, 32, 32, 512)\n",
    "            upsample(128, 4), # (bs, 64, 64, 256)\n",
    "            upsample(64, 4), # (bs, 128, 128, 128)\n",
    "        ]\n",
    "\n",
    "        self.last = tf.keras.layers.Conv2DTranspose(3, 4, strides=2,padding='same', kernel_initializer=tf.random_normal_initializer(0, 0.02), activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = inputs\n",
    "\n",
    "        skips = []\n",
    "        for down in self.down_stack:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "        output = self.last(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_237 (Sequential)  multiple                  3072      \n",
      "_________________________________________________________________\n",
      "sequential_238 (Sequential)  multiple                  131584    \n",
      "_________________________________________________________________\n",
      "sequential_239 (Sequential)  multiple                  525312    \n",
      "_________________________________________________________________\n",
      "sequential_240 (Sequential)  multiple                  2099200   \n",
      "_________________________________________________________________\n",
      "sequential_241 (Sequential)  multiple                  4196352   \n",
      "_________________________________________________________________\n",
      "sequential_242 (Sequential)  multiple                  4196352   \n",
      "_________________________________________________________________\n",
      "sequential_243 (Sequential)  multiple                  4196352   \n",
      "_________________________________________________________________\n",
      "sequential_244 (Sequential)  multiple                  4196352   \n",
      "_________________________________________________________________\n",
      "sequential_245 (Sequential)  multiple                  4196352   \n",
      "_________________________________________________________________\n",
      "sequential_246 (Sequential)  multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "sequential_247 (Sequential)  multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "sequential_248 (Sequential)  multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "sequential_249 (Sequential)  multiple                  4195328   \n",
      "_________________________________________________________________\n",
      "sequential_250 (Sequential)  multiple                  1049088   \n",
      "_________________________________________________________________\n",
      "sequential_251 (Sequential)  multiple                  262400    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_119 (Conv2D multiple                  6147      \n",
      "=================================================================\n",
      "Total params: 54,425,859\n",
      "Trainable params: 54,414,979\n",
      "Non-trainable params: 10,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.build(input_shape=(None, 256, 256, 3))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "LAMDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target-gen_output))\n",
    "    total_gen_loss = gan_loss + (LAMDA * l1_loss)\n",
    "\n",
    "    return total_gan_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        self.down_stack = [downsample(64, 4, False), downsample(128, 4), downsample(256, 4)]\n",
    "        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=tf.random_normal_initializer(0, 0.02), use_bias=False)\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU()\n",
    "        self.last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=tf.random_normal_initializer(0, 0.02))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs = input + target(concatenated)\n",
    "        x = inputs\n",
    "        for down in self.down_stack:\n",
    "            x = down(x)\n",
    "        x = self.conv(self.zero_pad1(x))\n",
    "        x = self.zero_pad2(self.leaky_relu(self.batchnorm(x)))\n",
    "        output = self.last(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_252 (Sequential)  multiple                  3072      \n",
      "_________________________________________________________________\n",
      "sequential_253 (Sequential)  multiple                  131584    \n",
      "_________________________________________________________________\n",
      "sequential_254 (Sequential)  multiple                  525312    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin multiple                  0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          multiple                  2097152   \n",
      "_________________________________________________________________\n",
      "batch_normalization_238 (Bat multiple                  2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_147 (LeakyReLU)  multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          multiple                  8193      \n",
      "=================================================================\n",
      "Total params: 2,767,361\n",
      "Trainable params: 2,765,569\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator()\n",
    "disc.build(input_shape=(None, 256, 256, 3))\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), desc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "    log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_input1 = tf.keras.layers.Concatenate()[input_image, target]\n",
    "        disc_input1 = tf.keras.layers.Concatenate()[input_image, gen_output]\n",
    "\n",
    "        disc_real_output = discriminator(disc_input1, training=True)\n",
    "        disc_generated_output = discriminator(disx_input2, training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "model.build((None, 28, 28))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = reversed(a[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/naoki/anaconda3/envs/tensorflow-new/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Tensorflow-new",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "tensorflow-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "all.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
